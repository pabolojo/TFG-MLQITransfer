{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.environments import tf_py_environment\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.policies import random_tf_policy\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload \n",
    "from QTransferEnv import *\n",
    "from QTransferLib import compute_populations, dephase_factory\n",
    "from AgentsTraining import *\n",
    "from Plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'logs/'\n",
    "root_dir = os.path.expanduser(root_dir)\n",
    "train_dir = os.path.join(root_dir, 'train')\n",
    "eval_dir = os.path.join(root_dir, 'eval')\n",
    "\n",
    "N = 5\n",
    "Ωmax = 1\n",
    "n_steps = 30\n",
    "γ = 0.\n",
    "t_max = 2\n",
    "initial_state = qt.basis(N, 0)\n",
    "target_state = qt.basis(N, N-1)\n",
    "noise = Noise(\"gaussian\", percentage=0.05)\n",
    "deltas = np.zeros(N, dtype=complex)\n",
    "deltas[1] = 0 + 0j\n",
    "\n",
    "num_iterations = 100\n",
    "reward_gain = 1.0\n",
    "summaries_flush_secs = 1\n",
    "num_eval_episodes = 10\n",
    "\n",
    "fc_layer_params=(100, 50, 30)\n",
    "optimizer_learning_rate=1e-3\n",
    "replay_buffer_episodes_capacity=10\n",
    "replay_buffer_capacity=replay_buffer_episodes_capacity * n_steps\n",
    "collect_episodes_per_iteration=1\n",
    "use_tf_functions=True\n",
    "batch_size=256\n",
    "train_steps_per_iteration=1\n",
    "replay_buffer_percentage_used_as_experience=1\n",
    "experience_episodes_per_train_step=int(replay_buffer_episodes_capacity*replay_buffer_percentage_used_as_experience)\n",
    "initial_collect_episodes=experience_episodes_per_train_step\n",
    "\n",
    "eval_interval=10\n",
    "summary_interval=10\n",
    "\n",
    "train_checkpoint_interval=num_iterations + 1\n",
    "policy_checkpoint_interval=num_iterations + 1\n",
    "rb_checkpoint_interval=num_iterations + 1\n",
    "log_interval=num_iterations + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train_py = QTransferEnv(N=N,\n",
    "                            t_max=t_max,\n",
    "                            n_steps=n_steps,\n",
    "                            initial_state=initial_state,\n",
    "                            target_state=target_state,\n",
    "                            reward_gain=reward_gain,\n",
    "                            omega_min=0,\n",
    "                            omega_max=Ωmax,\n",
    "                            noise=noise,\n",
    "                            deltas=deltas)\n",
    "\n",
    "env_eval_py = QTransferEnv(N=N,\n",
    "                           t_max=t_max,\n",
    "                           n_steps=n_steps,\n",
    "                           initial_state=initial_state,\n",
    "                           target_state=target_state,\n",
    "                           reward_gain=reward_gain,\n",
    "                           omega_min=0,\n",
    "                           omega_max=Ωmax,\n",
    "                           noise=noise,\n",
    "                           deltas=deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.compat.v1.train.get_or_create_global_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_env = tf_py_environment.TFPyEnvironment(env_train_py)\n",
    "eval_tf_env = tf_py_environment.TFPyEnvironment(env_eval_py)\n",
    "\n",
    "time_step_spec = tf_env.time_step_spec()\n",
    "observation_spec = time_step_spec.observation\n",
    "action_spec = tf_env.action_spec()\n",
    "\n",
    "# Actor Network\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    observation_spec,\n",
    "    action_spec,\n",
    "    fc_layer_params=fc_layer_params,\n",
    ")\n",
    "\n",
    "# Agent\n",
    "tf_agent = reinforce_agent.ReinforceAgent(\n",
    "    time_step_spec,\n",
    "    action_spec,\n",
    "    actor_network=actor_net,\n",
    "    optimizer=tf.compat.v1.train.AdamOptimizer(\n",
    "        learning_rate=optimizer_learning_rate),\n",
    "    normalize_returns=True,\n",
    "    train_step_counter=global_step,\n",
    ")\n",
    "\n",
    "tf_agent.initialize()\n",
    "\n",
    "# define the optimizer\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)  # (learning_rate=learning_rate)\n",
    "\n",
    "# Replay Buffer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=tf_agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=replay_buffer_capacity,\n",
    ")\n",
    "replay_observer = [replay_buffer.add_batch]\n",
    "\n",
    "# Train Metrics\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(\n",
    "        buffer_size=num_eval_episodes, batch_size=tf_env.batch_size),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(\n",
    "        buffer_size=num_eval_episodes, batch_size=tf_env.batch_size),\n",
    "]\n",
    "\n",
    "# Eval Metrics\n",
    "eval_metrics = [\n",
    "    tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n",
    "]\n",
    "\n",
    "# Policies\n",
    "eval_policy = tf_agent.policy\n",
    "initial_collect_policy = random_tf_policy.RandomTFPolicy(time_step_spec, action_spec)\n",
    "collect_policy = tf_agent.collect_policy\n",
    "\n",
    "# Drivers\n",
    "initial_collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env,\n",
    "    initial_collect_policy,\n",
    "    observers=replay_observer + train_metrics,\n",
    "    num_episodes=initial_collect_episodes)\n",
    "\n",
    "collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "    tf_env,\n",
    "    collect_policy,\n",
    "    observers=replay_observer + train_metrics,\n",
    "    num_episodes=collect_episodes_per_iteration)\n",
    "\n",
    "eval_driver = dynamic_episode_driver.DynamicEpisodeDriver(eval_tf_env,\n",
    "                                                          eval_policy,\n",
    "                                                          eval_metrics,\n",
    "                                                          num_episodes=1)\n",
    "\n",
    "if use_tf_functions:\n",
    "    initial_collect_driver.run = common.function(initial_collect_driver.run)\n",
    "    collect_driver.run = common.function(collect_driver.run)\n",
    "    eval_driver.run = common.function(eval_driver.run)\n",
    "    tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "if replay_buffer.num_frames() == 0:\n",
    "    # Collect initial replay data.\n",
    "    initial_collect_driver.run()\n",
    "\n",
    "time_step = None\n",
    "policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n",
    "\n",
    "timed_at_step = global_step.numpy()\n",
    "time_acc = 0\n",
    "\n",
    "def train_step():\n",
    "        experience = replay_buffer.gather_all()\n",
    "        return tf_agent.train(experience)\n",
    "\n",
    "if use_tf_functions:\n",
    "    train_step = common.function(train_step)\n",
    "\n",
    "global_step_val = global_step.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the agent's policy once before training.\n",
    "final_time_step, policy_state = eval_driver.run()\n",
    "print(\"Initial Average Return: \", eval_metrics[0].result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_list = []\n",
    "episode_list = []\n",
    "iteration_list = []\n",
    "with trange(num_iterations, dynamic_ncols=False) as t:\n",
    "    for i in t:\n",
    "        t.set_description(f'episode {i}')\n",
    "\n",
    "        time_step, policy_state = collect_driver.run(\n",
    "            time_step=time_step,\n",
    "            policy_state=policy_state,\n",
    "        )\n",
    "        \n",
    "        train_loss = train_step()\n",
    "\n",
    "        if i % eval_interval == 0 or i == num_iterations - 1:\n",
    "            eval_metrics[0].reset()\n",
    "            _ = eval_driver.run()\n",
    "\n",
    "            t.set_postfix({\"return\": eval_metrics[0].result().numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_list = []\n",
    "episode_list = []\n",
    "iteration_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_returns(return_list, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = env_eval_py.times\n",
    "def interactive(Iteration):\n",
    "    plot_episode(times, episode_list[Iteration])\n",
    "interact(interactive, Iteration=widgets.IntSlider(min=0, max=len(episode_list)-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Population {}: {}\".format(N, episode_list[-1].observation.numpy()[0, -1, N - 1]))\n",
    "print(\"Max population: {}\".format(np.max(episode_list[-1].observation.numpy()[0, :, N - 1])))\n",
    "print(\"Total population: {}\".format(np.sum(episode_list[-1].observation.numpy()[0, -1, :N])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive(Iteration):\n",
    "    pulses = np.array(episode_list[Iteration].action.numpy()[0, :, :])\n",
    "    ax = plot_pulses(times, pulses)\n",
    "    ax.set_ylim((0, Ωmax*1.05))\n",
    "interact(interactive, Iteration=widgets.IntSlider(min=0, max=len(episode_list)-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive(Iteration):\n",
    "    pulses = np.array(episode_list[Iteration].action.numpy()[0, :, :])\n",
    "    dm = env_eval_py.run_qstepevolution(pulses)\n",
    "    populations = np.diagonal(dm, axis1=1, axis2=2).real\n",
    "    plot_populations(env_eval_py.times, populations)\n",
    "    print(\"Population {}: {}\".format(N, populations[-1][-1]))\n",
    "interact(interactive, Iteration=widgets.IntSlider(min=0, max=len(episode_list)-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive(Iteration):\n",
    "    pulses = np.array(episode_list[Iteration].action.numpy()[0, :, :])\n",
    "    times, populations = compute_populations(N, initial_state, t_max, pulses, constant_delta_factory(deltas, n_steps + 1), dephase_factory(N, γ))\n",
    "    plot_populations(times, populations)\n",
    "    print(\"Population {}: {}\".format(N, populations[-1][-1]))\n",
    "interact(interactive, Iteration=widgets.IntSlider(min=0, max=len(episode_list)-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
